{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23509,"status":"ok","timestamp":1709890890418,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"C_kCVPKV2yct","outputId":"936a4e92-59ed-4872-cac9-02e359a8db4a"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709890890418,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"ueEwT9rj21Zq","outputId":"a1499343-52ab-454f-ee32-4a0f43a100a2"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1365,"status":"ok","timestamp":1709890891776,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"hcTal8N0HU9o"},"outputs":[],"source":["import numpy as np\n","import random\n","import pandas as pd\n","import os\n","import time\n","import librosa\n","import librosa.display\n","import IPython\n","from IPython.display import Audio\n","from IPython.display import Image\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sys\n","\n","import warnings\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3128,"status":"ok","timestamp":1709890894892,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"wqVbg_BgHYq-"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class FeatureEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv1d(1, 256, kernel_size=5, stride=1) # [batch size, 1, 193] -> [batch size, 256, 189]\n","        self.bn1 = nn.BatchNorm1d(256)\n","        self.relu1 = nn.ReLU()\n","\n","        self.conv2 = nn.Conv1d(256, 128, kernel_size=5, stride=1) # [batch size, 256, 189] -> [batch size, 128, 185]\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.relu2 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.1)\n","\n","        self.conv3 = nn.Conv1d(128, 128, kernel_size=5, stride=1) # [batch size, 128, 185] -> [batch size, 128, 181]\n","        self.bn3 = nn.BatchNorm1d(128)\n","        self.relu3 = nn.ReLU()\n","\n","    def forward(self, x):\n","        # Convolutional layers\n","        conv_embedding = self.relu1(self.bn1(self.conv1(x)))\n","        conv_embedding = self.dropout1(self.relu2(self.bn2(self.conv2(conv_embedding))))\n","        conv_embedding = self.relu3(self.bn3(self.conv3(conv_embedding)))\n","        return conv_embedding\n","\n","class EmotionClassifier(nn.Module):\n","    def __init__(self, num_emotions):\n","        super().__init__()\n","\n","        self.conv4 = nn.Conv1d(128, 128, kernel_size=5, stride=1) # [batch size, 128, 181] -> [batch size, 128, 177]\n","        self.bn4 = nn.BatchNorm1d(128)\n","        self.relu4 = nn.ReLU()\n","        self.maxpool = nn.MaxPool1d(kernel_size=8) # [batch size, 128, 177] -> [batch size, 128, 22]\n","\n","        self.conv5 = nn.Conv1d(128, 128, kernel_size=5, stride=1)\n","        self.bn5 = nn.BatchNorm1d(128)\n","        self.relu5 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(0.2)\n","\n","        self.conv6 = nn.Conv1d(128, 128, kernel_size=5, stride=1)\n","        self.bn6 = nn.BatchNorm1d(128)\n","        self.relu6 = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","        self.dropout3 = nn.Dropout(0.2)\n","\n","        self.emotion_classifier = nn.Sequential(\n","            nn.Linear(1792, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, num_emotions),\n","        )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        # Convolutional layers\n","        conv_embedding = self.maxpool(self.relu4(self.bn4(self.conv4(x))))\n","        conv_embedding = self.dropout2(self.relu5(self.bn5(self.conv5(conv_embedding))))\n","        conv_embedding = self.relu6(self.bn6(self.conv6(conv_embedding)))\n","        # Flatten\n","        conv_embedding = self.flatten(conv_embedding)\n","        conv_embedding = self.dropout3(conv_embedding)\n","        output_logits = self.emotion_classifier(conv_embedding)\n","        output_softmax = self.softmax(output_logits)\n","\n","        return output_logits, output_softmax\n","\n","class Critic(nn.Module): # Domain discriminator\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv4 = nn.Conv1d(128, 128, kernel_size=5, stride=1) # [batch size, 128, 181] -> [batch size, 128, 177]\n","        self.bn4 = nn.BatchNorm1d(128)\n","        self.relu4 = nn.ReLU()\n","        self.maxpool = nn.MaxPool1d(kernel_size=8) # [batch size, 128, 177] -> [batch size, 128, 22]\n","\n","        self.conv5 = nn.Conv1d(128, 128, kernel_size=5, stride=1)\n","        self.bn5 = nn.BatchNorm1d(128)\n","        self.relu5 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(0.2)\n","\n","        self.conv6 = nn.Conv1d(128, 128, kernel_size=5, stride=1)\n","        self.bn6 = nn.BatchNorm1d(128)\n","        self.relu6 = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","        self.dropout3 = nn.Dropout(0.2)\n","\n","        self.critic = nn.Sequential(\n","            nn.Linear(1792, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1),  # Linear activation for critic\n","            nn.Sigmoid() # between 0 and 1\n","        )\n","\n","    def forward(self, x):\n","        conv_embedding = self.maxpool(self.relu4(self.bn4(self.conv4(x))))\n","        conv_embedding = self.dropout2(self.relu5(self.bn5(self.conv5(conv_embedding))))\n","        conv_embedding = self.relu6(self.bn6(self.conv6(conv_embedding)))\n","        # Flatten\n","        conv_embedding = self.flatten(conv_embedding)\n","        conv_embedding = self.dropout3(conv_embedding)\n","        return self.critic(conv_embedding)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709890894892,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"J1rUjNu6GwVw"},"outputs":[],"source":["import random\n","\n","def invert_subset(labels_tensor, flip_prob):\n","    \"\"\"\n","    Flip a random subset of numbers in a 1D tensor.\n","\n","    Args:\n","    - tensor (torch.Tensor): Input 1D tensor containing only zeroes and ones.\n","    - flip_prob (float): Probability of flipping each element in the tensor.\n","\n","    Returns:\n","    - torch.Tensor: Modified tensor with a random subset of elements flipped.\n","    \"\"\"\n","    # Ensure the tensor contains only zeros and ones\n","    assert torch.all(torch.logical_or(labels_tensor == 0, labels_tensor == 1)), \"Input tensor must contain only zeroes and ones.\"\n","\n","    # Generate a mask for flipping elements based on flip_prob\n","    flip_mask = (torch.rand_like(labels_tensor) < flip_prob).float()\n","\n","    # Flip the selected elements (0 becomes 1, and 1 becomes 0)\n","    flipped_tensor = torch.abs(labels_tensor - flip_mask)\n","\n","    return flipped_tensor"]},{"cell_type":"markdown","metadata":{},"source":["## Load arrays"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9610,"status":"ok","timestamp":1709890904485,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"toE6pSNMgrl3","outputId":"1947b903-6ffa-43c2-e4ef-2ed74c9fbb81"},"outputs":[],"source":["X_train = np.load(file=\"/content/drive/MyDrive/  /xtrain.npy\")\n","X_test = np.load(file=\"/content/drive/MyDrive/  /xtest.npy\")\n","X_val = np.load(file=\"/content/drive/MyDrive/  /xval.npy\")\n","\n","Y_train = np.load(file=\"/content/drive/MyDrive/  /ytrain.npy\")\n","Y_test = np.load(file=\"/content/drive/MyDrive/  /ytest.npy\")\n","Y_val = np.load(file=\"/content/drive/MyDrive/  /yval.npy\")\n","\n","Z_train = np.load(file=\"/content/drive/MyDrive/  /ztrain.npy\")\n","Z_test = np.load(file=\"/content/drive/MyDrive/  /ztest.npy\")\n","Z_val = np.load(file=\"/content/drive/MyDrive/  /zval.npy\")\n","\n","print(X_train.shape, Y_train.shape, Z_train.shape)\n","print(X_test.shape, Y_test.shape, Z_test.shape)\n","print(X_val.shape, Y_val.shape, Z_val.shape)\n","# random.shuffle(Y_train)\n","# random.shuffle(Y_train)\n","# random.shuffle(Y_train)\n","# random.shuffle(Y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1709890904485,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"z23jyi23Fx_P"},"outputs":[],"source":["# has_none = np.any(np.isnan(Y_train))\n","# if has_none:\n","#     print(\"The array contains at least one None value.\")\n","# else:\n","#     print(\"The array does not contain None values.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Set hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1718,"status":"ok","timestamp":1709890910221,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"yLk8jpk8F8Sl","outputId":"983280b9-5234-4865-eb3a-17c8ba28ed66"},"outputs":[],"source":["EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'}\n","EPOCHS = 250\n","DATASET_SIZE = X_train.shape[0]\n","BATCH_SIZE = 64\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Selected device is {}'.format(device))\n","\n","# Instantiate models\n","feature_encoder = FeatureEncoder().to(device)\n","emotion_classifier = EmotionClassifier(len(EMOTIONS)).to(device)\n","critic = Critic().to(device)\n","\n","# Define loss functions and optimizers\n","classification_loss_fn = nn.CrossEntropyLoss()\n","domain_discrimination_loss_fn = nn.BCELoss() # Binary loss when there are only 2 domains\n","feature_encoder_optimizer = optim.Adam(feature_encoder.parameters(), lr=0.0001, weight_decay=1e-5)\n","emotion_classifier_optimizer = optim.Adam(emotion_classifier.parameters(), lr=0.0001, weight_decay=1e-5)\n","critic_optimizer = optim.Adam(critic.parameters(), lr=0.0001, weight_decay=1e-5)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":564983,"status":"ok","timestamp":1709891480017,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"mtwPumfv2300","outputId":"74ebe9ea-8f8b-4641-a689-33e93e5b8f9a"},"outputs":[],"source":["training_classification_losses = []\n","training_domain_disc_losses = []\n","\n","val_classification_losses = []\n","val_domain_disc_losses = []\n","\n","start = time.time()\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    # shuffle data\n","    start_epoch = time.time()\n","    ind = np.random.permutation(DATASET_SIZE)\n","    X_train = X_train[ind,:,:]\n","    Y_train = Y_train[ind]\n","    Z_train = Z_train[ind]\n","    emotion_classification_acc = 0\n","    emotion_classification_loss = 0\n","    domain_disc_acc = 0\n","    epoch_domain_disc_loss = 0\n","\n","    iters = int(DATASET_SIZE / BATCH_SIZE)\n","\n","    feature_encoder.train()\n","    emotion_classifier.train()\n","    critic.train()\n","\n","    for i in range(iters):\n","\n","        ### ----- Setting up batch ----- ###\n","        batch_start = i * BATCH_SIZE\n","        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n","        actual_batch_size = batch_end-batch_start\n","        X = X_train[batch_start:batch_end,:,:]\n","        Y = Y_train[batch_start:batch_end]\n","        Z = Z_train[batch_start:batch_end]\n","        X_tensor = torch.tensor(X, device=device).float() # mel_spectrogram\n","        Y_tensor = torch.tensor(Y, dtype=torch.long, device=device) # emotion label\n","        Z_tensor = torch.tensor(Z, device=device).float() # domain label\n","\n","        ### ----- Training step ----- ###\n","        # Forward pass\n","        features = feature_encoder(X_tensor)\n","        emotion_logits, emotion_softmax = emotion_classifier(features.detach()) # [64, 8] , .detach() creates a copy of the tensor that does not require gradients\n","                                                                                # This DOES NOT change the requires_grad attribute of the original tensor or parameters of the model\n","                                                                                # logits are used for cross entropy loss\n","                                                                                # softmax used for actual prediction (0-7)\n","\n","        domain_predictions = critic(features.detach())\n","        domain_predictions_squeezed = domain_predictions.squeeze(dim=1).clone() # [64, 1] to [64] , .clone() to prevent in-place operations\n","        domain_predictions2 = critic(features) # feature encoder adversarial training\n","        domain_predictions2_squeezed = domain_predictions2.squeeze(dim=1).clone()\n","\n","        # Compute losses\n","        classification_loss = classification_loss_fn(emotion_logits, Y_tensor) # Y_tensor.shape [64]\n","        domain_discrimination_loss = domain_discrimination_loss_fn(domain_predictions_squeezed, Z_tensor)\n","\n","        # Backpropagation and optimization for domain discriminator (critic)\n","        domain_discrimination_loss.backward() # critic only\n","        # critic_optimizer.step()\n","        # critic_optimizer.zero_grad()\n","\n","        # Backpropagation and optimization for emotion classifier\n","        classification_loss.backward() # emotion classifier only\n","        emotion_classifier_optimizer.step()\n","        emotion_classifier_optimizer.zero_grad()\n","\n","        # Adversarial training for feature extractor (Inverting domain labels to fool the discriminator)\n","        invert_prob = 0.7 # probability of flipping\n","        inverted_domain_labels = invert_subset(Z_tensor, invert_prob)\n","        adversarial_loss = domain_discrimination_loss_fn(domain_predictions2_squeezed, inverted_domain_labels)\n","        adversarial_loss.backward() # critic + feature encoder\n","        feature_encoder_optimizer.step()\n","        feature_encoder_optimizer.zero_grad()\n","\n","        ### ----- Tracking training loss per epoch ----- ###\n","        # Emotion classification\n","        actual_prediction = torch.argmax(emotion_softmax, dim=1) # Actual prediction (0-7)\n","        prediction_accuracy = (torch.sum(Y_tensor == actual_prediction)/float(len(Y_tensor))) * 100\n","        emotion_classification_acc += prediction_accuracy*actual_batch_size/DATASET_SIZE\n","        emotion_classification_loss += classification_loss*actual_batch_size/DATASET_SIZE\n","\n","        # Domain discrimination\n","        predicted_domain_labels = (domain_predictions_squeezed > 0.5).long()\n","        correct_domain_predictions = (predicted_domain_labels == Z_tensor.long()).float()\n","        batch_domain_disc_acc = (torch.sum(correct_domain_predictions) / len(Z_tensor)) * 100\n","        domain_disc_acc += batch_domain_disc_acc*actual_batch_size/DATASET_SIZE\n","        epoch_domain_disc_loss += domain_discrimination_loss*actual_batch_size/DATASET_SIZE\n","        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n","\n","        # break\n","\n","    ### ----- Validation step ----- ###\n","    X_val_tensor = torch.tensor(X_val, device=device).float()\n","    Y_val_tensor = torch.tensor(Y_val, dtype=torch.long, device=device)\n","    Z_val_tensor = torch.tensor(Z_val, device=device).float()\n","\n","    feature_encoder.eval()\n","    emotion_classifier.eval()\n","    critic.eval()\n","\n","    with torch.no_grad():\n","        features = feature_encoder(X_val_tensor)\n","        emotion_logits, emotion_softmax = emotion_classifier(features)\n","        domain_predictions = critic(features)\n","        domain_predictions_squeezed = domain_predictions.squeeze(dim=1).clone()\n","\n","        val_classification_loss = classification_loss_fn(emotion_logits, Y_val_tensor)\n","        val_domain_discrimination_loss = domain_discrimination_loss_fn(domain_predictions_squeezed, Z_val_tensor)\n","\n","        ### ----- Tracking validation loss per epoch ----- ###\n","        # Validation emotion classification\n","        actual_prediction = torch.argmax(emotion_softmax, dim=1)\n","        val_prediction_acc = (torch.sum(Y_val_tensor == actual_prediction)/float(len(Y_val_tensor))) * 100\n","\n","        # Validation domain discrimination\n","        predicted_domain_labels = (domain_predictions_squeezed > 0.5).long()\n","        correct_domain_predictions = (predicted_domain_labels == Z_val_tensor.long()).float()\n","        val_domain_disc_acc = (torch.sum(correct_domain_predictions) / len(Z_val_tensor)) * 100\n","\n","    ### ----- Tracking loss over time to graph ----- ###\n","    training_classification_losses.append(emotion_classification_loss.cpu())\n","    training_domain_disc_losses.append(epoch_domain_disc_loss)\n","    val_classification_losses.append(val_classification_loss.cpu())\n","    val_domain_disc_losses.append(val_domain_discrimination_loss)\n","    elapsed_epoch = time.time() - start_epoch\n","    print('')\n","    print(f\" Epoch {epoch} --> Emotion classification acc: {emotion_classification_acc:.2f}%, Emotion classification loss: {emotion_classification_loss:.4f}, \\\n","             Domain prediction acc: {domain_disc_acc:.2f}%, Domain discrimination loss: {epoch_domain_disc_loss:.4f}, \\n\\\n","             Val classification acc: {val_prediction_acc:.4f}%, Val classification loss: {val_classification_loss:.2f}, \\\n","             \\t    Val domain acc: {val_domain_disc_acc:.4f}%, Val domain loss: {val_domain_discrimination_loss:.2f}, \\\n","             \\t time:{elapsed_epoch:.2f}sec \\n\")\n","    # break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1709891529558,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"vh4y9Ihe92oB","outputId":"c58b71b6-7f43-49ae-f718-f35243da1fe0"},"outputs":[],"source":["elapsed = time.time() - start\n","print(f\"Total training time:{elapsed:.2f}sec\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":739},"executionInfo":{"elapsed":5,"status":"error","timestamp":1709641666534,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"I-1aFP8G931K","outputId":"70408165-a313-4dbc-a141-4dab2b6497e5"},"outputs":[],"source":["# plt.plot(training_classification_losses,'b')\n","# plt.plot(val_classification_losses,'r')\n","# plt.legend(['train loss','val loss'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":483,"status":"ok","timestamp":1709891538521,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"K83SqqBq91Qv","outputId":"1939e2a6-2b8c-4345-8db2-8b32becccb2c"},"outputs":[],"source":["SAVE_PATH = os.path.join(os.getcwd(),'/content/drive/MyDrive/ ')\n","# os.makedirs('models',exist_ok=True)\n","torch.save(feature_encoder.state_dict(),os.path.join(SAVE_PATH,'feature_encoder.pt'))\n","torch.save(emotion_classifier.state_dict(),os.path.join(SAVE_PATH,'emotion_classifier.pt'))\n","torch.save(critic.state_dict(),os.path.join(SAVE_PATH,'critic.pt'))\n","print('Models are saved to {}'.format(os.path.join(SAVE_PATH,'feature_encoder.pt')))\n","print('Models are saved to {}'.format(os.path.join(SAVE_PATH,'emotion_classifier.pt')))\n","print('Models are saved to {}'.format(os.path.join(SAVE_PATH,'critic.pt')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709891541432,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"uZnk2XVY91J6","outputId":"568c42f4-5ed7-4a79-80ef-075d11c87500"},"outputs":[],"source":["# Load models\n","LOAD_PATH = os.path.join(os.getcwd(),'/content/drive/MyDrive/ ')\n","\n","feature_encoder.load_state_dict(torch.load(os.path.join(LOAD_PATH,'feature_encoder.pt')))\n","print('Feature encoder is loaded from {}'.format(os.path.join(LOAD_PATH,'feature_encoder.pt')))\n","\n","emotion_classifier.load_state_dict(torch.load(os.path.join(LOAD_PATH,'emotion_classifier.pt')))\n","print('Emotion classifier is loaded from {}'.format(os.path.join(LOAD_PATH,'emotion_classifier.pt')))\n","\n","critic.load_state_dict(torch.load(os.path.join(LOAD_PATH,'critic.pt')))\n","print('Critic is loaded from {}'.format(os.path.join(LOAD_PATH,'critic.pt')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1858,"status":"ok","timestamp":1709891548226,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"I3DkM4rt906n","outputId":"c7e80b0c-faee-48c8-aeba-ed68ffbe4064"},"outputs":[],"source":["Ravdess_X_test = np.load(file=\"/content/drive/MyDrive/  /Ravdess_xtest.npy\")\n","Ravdess_Y_test = np.load(file=\"/content/drive/MyDrive/  /Ravdess_ytest.npy\")\n","Ravdess_Z_test = np.load(file=\"/content/drive/MyDrive/  /Ravdess_ztest.npy\")\n","\n","print(Ravdess_X_test.shape)\n","print(Ravdess_Y_test.shape)\n","print(Ravdess_Z_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":529,"status":"ok","timestamp":1709891571871,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"FNRjfdh2e4EP","outputId":"a204b930-01b3-4718-ed1c-8215c90f75c1"},"outputs":[],"source":["start = time.time()\n","\n","X_test_tensor = torch.tensor(Ravdess_X_test, device=device).float()\n","Y_test_tensor = torch.tensor(Ravdess_Y_test, dtype=torch.long, device=device)\n","Z_test_tensor = torch.tensor(Ravdess_Z_test, device=device).float()\n","\n","feature_encoder.eval()\n","emotion_classifier.eval()\n","critic.eval()\n","\n","with torch.no_grad():\n","    features = feature_encoder(X_test_tensor)\n","    emotion_logits, emotion_softmax = emotion_classifier(features)\n","    domain_predictions = critic(features)\n","    domain_predictions_squeezed = domain_predictions.squeeze(dim=1).clone()\n","\n","    val_classification_loss = classification_loss_fn(emotion_logits, Y_test_tensor)\n","    val_domain_discrimination_loss = domain_discrimination_loss_fn(domain_predictions_squeezed, Z_test_tensor)\n","\n","    ### ----- Tracking validation loss per epoch ----- ###\n","    # Validation emotion classification\n","    actual_prediction = torch.argmax(emotion_softmax, dim=1)\n","    val_prediction_acc = (torch.sum(Y_test_tensor == actual_prediction)/float(len(Y_test_tensor))) * 100\n","\n","    # Validation domain discrimination\n","    predicted_domain_labels = (domain_predictions_squeezed > 0.5).long()\n","    correct_domain_predictions = (predicted_domain_labels == Z_test_tensor.long()).float()\n","    val_domain_disc_acc = (torch.sum(correct_domain_predictions) / len(Z_test_tensor)) * 100\n","\n","elapsed = time.time() - start\n","print(f\"Total test time:{elapsed:.3f}sec\")\n","print(f'Emotion classification Test loss is {val_classification_loss:.3f}')\n","print(f'Emotion classification Test accuracy is {val_prediction_acc:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"executionInfo":{"elapsed":737,"status":"ok","timestamp":1709891577147,"user":{"displayName":"るい","userId":"15353790188108276038"},"user_tz":-480},"id":"Kv3O-tlHngoX","outputId":"1a01ee7b-9c29-4a35-be9b-41105d1b382a"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","\n","actual_prediction = actual_prediction.cpu().numpy()\n","cm = confusion_matrix(Ravdess_Y_test, actual_prediction)\n","names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n","df_cm = pd.DataFrame(cm, index=names, columns=names)\n","# plt.figure(figsize=(10,7))\n","sn.set(font_scale=1.4) # for label size\n","sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g') # font size\n","plt.show()\n","\n","# Rows are the inputs, columns are the classification"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMdKdKmj9It4sSP6/Nt1220","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
